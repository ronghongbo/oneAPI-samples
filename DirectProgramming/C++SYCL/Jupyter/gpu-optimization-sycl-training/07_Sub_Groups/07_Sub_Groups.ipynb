{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sub-groups\n",
    "The index space of an ND-Range kernel is divided into work-groups, sub-groups, and work-items. A work-item is the basic unit. A collection of work-items form a sub-group, and a collection of sub-groups form a work-group. The mapping of work-items and work-groups to hardware execution units (EU) is implementation-dependent. All the work-groups run concurrently but may be scheduled to run at different times depending on availability of resources. Work-group execution may or or may not be preempted depending on the capabilities of underlying hardware. Work-items in the same work-group are guaranteed to run concurrently. Work-items in the same sub-group may have additional scheduling guarantees and have access to additional functionality.\n",
    "\n",
    "A sub-group is a collection of contiguous work-items in the global index space that execute in the same EU thread. When the device compiler compiles the kernel, multiple work-items are packed into a sub-group by vectorization so the generated SIMD instruction stream can perform tasks of multiple work-items simultaneously. Properly partitioning work-items into sub-groups can make a big performance difference.\n",
    "\n",
    "In this section we cover performance impact and consideration when writing kernel with Sub-Groups:\n",
    "\n",
    "- [Sub-group Sizes](#Sub-group-Sizes)\n",
    "- [Sub-group Size vs. Maximum Sub-group Size](#Sub-group-Size-vs.-Maximum-Sub-group-Size)\n",
    "- [Vectorization and Memory Access](#Vectorization-and-Memory-Access)\n",
    "- [Data Sharing](#Data-Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69230766-332c-4b99-8abe-38cdd62d1c56",
   "metadata": {},
   "source": [
    "## Sub-group Sizes\n",
    "\n",
    "By default, the compiler selects a sub-group size using device-specific information and a few heuristics. The user can override the compiler’s selection using the kernel attribute `intel::reqd_sub_group_size` to specify the maximum sub-group size. Sometimes, not always, explicitly requesting a sub-group size may help performance.\n",
    "\n",
    "The code below is sub-group example which prints sub-group information for each work-item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725a45ce-25c0-4b43-a986-483324e4d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_size.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_size.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>()<< \"\\n\";\n",
    "\n",
    "  q.submit([&](auto &h) {\n",
    "    sycl::stream out(65536, 256, h);\n",
    "    h.parallel_for(sycl::nd_range<1>(32,32), [=](sycl::nd_item<1> it) {\n",
    "         int groupId = it.get_group(0);\n",
    "         int globalId = it.get_global_linear_id();\n",
    "         auto sg = it.get_sub_group();\n",
    "         int sgSize = sg.get_local_range()[0];\n",
    "         int sgGroupId = sg.get_group_id()[0];\n",
    "         int sgId = sg.get_local_id()[0];\n",
    "\n",
    "         out << \"globalId = \" << sycl::setw(2) << globalId\n",
    "             << \" groupId = \" << groupId\n",
    "             << \" sgGroupId = \" << sgGroupId << \" sgId = \" << sgId\n",
    "             << \" sgSize = \" << sycl::setw(2) << sgSize\n",
    "             << sycl::endl;\n",
    "    });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8435-c445-4ca5-91e7-a72b6acbac46",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e014403-ac0b-43a6-abb0-0431b1449454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:02:40 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127231.v-qsvr-1           ...on_atomics.sh u49991                 0 R batch          \n",
      "2127232.v-qsvr-1           run_sg_size.sh   u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████████████████████████████████████████████\n",
      "\n",
      "TimeOut 60 seconds: Job is still queued for execution, check for output file later (run_sg_size.sh.o2127232)\n",
      "\n",
      " Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:18:15 AM PST\n",
      "#    Job ID:           2127232.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "globalId = 16 groupId =  0 sgGroupId =  1 sgId =  0 sgSize = 16\n",
      "globalId = 17 groupId =  0 sgGroupId =  1 sgId =  1 sgSize = 16\n",
      "globalId = 18 groupId =  0 sgGroupId =  1 sgId =  2 sgSize = 16\n",
      "globalId = 19 groupId =  0 sgGroupId =  1 sgId =  3 sgSize = 16\n",
      "globalId = 20 groupId =  0 sgGroupId =  1 sgId =  4 sgSize = 16\n",
      "globalId = 21 groupId =  0 sgGroupId =  1 sgId =  5 sgSize = 16\n",
      "globalId = 22 groupId =  0 sgGroupId =  1 sgId =  6 sgSize = 16\n",
      "globalId = 23 groupId =  0 sgGroupId =  1 sgId =  7 sgSize = 16\n",
      "globalId = 24 groupId =  0 sgGroupId =  1 sgId =  8 sgSize = 16\n",
      "globalId = 25 groupId =  0 sgGroupId =  1 sgId =  9 sgSize = 16\n",
      "globalId = 26 groupId =  0 sgGroupId =  1 sgId = 10 sgSize = 16\n",
      "globalId = 27 groupId =  0 sgGroupId =  1 sgId = 11 sgSize = 16\n",
      "globalId = 28 groupId =  0 sgGroupId =  1 sgId = 12 sgSize = 16\n",
      "globalId = 29 groupId =  0 sgGroupId =  1 sgId = 13 sgSize = 16\n",
      "globalId = 30 groupId =  0 sgGroupId =  1 sgId = 14 sgSize = 16\n",
      "globalId = 31 groupId =  0 sgGroupId =  1 sgId = 15 sgSize = 16\n",
      "globalId =  0 groupId =  0 sgGroupId =  0 sgId =  0 sgSize = 16\n",
      "globalId =  1 groupId =  0 sgGroupId =  0 sgId =  1 sgSize = 16\n",
      "globalId =  2 groupId =  0 sgGroupId =  0 sgId =  2 sgSize = 16\n",
      "globalId =  3 groupId =  0 sgGroupId =  0 sgId =  3 sgSize = 16\n",
      "globalId =  4 groupId =  0 sgGroupId =  0 sgId =  4 sgSize = 16\n",
      "globalId =  5 groupId =  0 sgGroupId =  0 sgId =  5 sgSize = 16\n",
      "globalId =  6 groupId =  0 sgGroupId =  0 sgId =  6 sgSize = 16\n",
      "globalId =  7 groupId =  0 sgGroupId =  0 sgId =  7 sgSize = 16\n",
      "globalId =  8 groupId =  0 sgGroupId =  0 sgId =  8 sgSize = 16\n",
      "globalId =  9 groupId =  0 sgGroupId =  0 sgId =  9 sgSize = 16\n",
      "globalId = 10 groupId =  0 sgGroupId =  0 sgId = 10 sgSize = 16\n",
      "globalId = 11 groupId =  0 sgGroupId =  0 sgId = 11 sgSize = 16\n",
      "globalId = 12 groupId =  0 sgGroupId =  0 sgId = 12 sgSize = 16\n",
      "globalId = 13 groupId =  0 sgGroupId =  0 sgId = 13 sgSize = 16\n",
      "globalId = 14 groupId =  0 sgGroupId =  0 sgId = 14 sgSize = 16\n",
      "globalId = 15 groupId =  0 sgGroupId =  0 sgId = 15 sgSize = 16\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127232.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:18:30 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 70 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_size.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb01f9-de33-43b8-9d08-3eef28675108",
   "metadata": {},
   "source": [
    "Each sub-group in this example has 16 work-items, or the sub-group size is 16. This means each thread simultaneously executes 16 work-items and 32 work-items are executed by two EU threads.\n",
    "\n",
    "The valid sub-group sizes are device dependent. You can query the device to get this information:\n",
    "```cpp\n",
    "  std::cout << \"Sub-group Sizes: \";\n",
    "  for (const auto &s :\n",
    "       q.get_device().get_info<sycl::info::device::sub_group_sizes>()) {\n",
    "    std::cout << s << \" \";\n",
    "  }\n",
    "  std::cout << std::endl;\n",
    "```\n",
    "The valid sub-group sizes supported may be:\n",
    "```\n",
    "Subgroup Sizes: 8 16 32\n",
    "```\n",
    "\n",
    "You can modify and check the output of the above code by overriding the compiler's selection of sub-group size, and set sub-group size to `32` using the kernel attribute `intel::reqd_sub_group_size`:\n",
    "```cpp\n",
    "    h.parallel_for(sycl::nd_range<1>(32,32), [=](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(32)]] {\n",
    "        // Kernel Code\n",
    "    });\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aa78e-49e3-4e6f-a83c-eebe436123d8",
   "metadata": {},
   "source": [
    "## Sub-group Size vs. Maximum Sub-group Size\n",
    "So far in our examples, the work-group size is divisible by the sub-group size and both the work-group size and the sub-group size (either required by the user or automatically picked by the compiler are powers of two). The sub-group size and maximum sub-group size are the same if the work-group size is divisible by the maximum sub-group size and both sizes are powers of two. But what happens if the work-group size is not divisible by the sub-group size? \n",
    "\n",
    "Consider the following example, the sub-group size is seven, though the maximum sub-group size is still eight! The maximum sub-group size is actually the SIMD width so it does not change, but there are less than eight work-items in the sub-group, so the sub-group size is seven. So be careful when your work-group size is not divisible by the maximum sub-group size. The last sub-group with fewer work-items may need to be specially handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c7a5f3-4d86-4149-9595-795147c9f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_max_size.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_max_size.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr int N = 15;\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>()<< \"\\n\";\n",
    "\n",
    "  int *data = sycl::malloc_shared<int>(N + N + 2, q);\n",
    "\n",
    "  for (int i = 0; i < N + N + 2; i++) {\n",
    "    data[i] = i;\n",
    "  }\n",
    "\n",
    "  // Snippet begin\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::stream out(65536, 128, h);\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range<1>(15, 15), [=](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "          int i = it.get_global_linear_id();\n",
    "          auto sg = it.get_sub_group();\n",
    "          int sgSize = sg.get_local_range()[0];\n",
    "          int sgMaxSize = sg.get_max_local_range()[0];\n",
    "          int sId = sg.get_local_id()[0];\n",
    "          int j = data[i];\n",
    "          int k = data[i + sgSize];\n",
    "          out << \"globalId = \" << i << \" sgMaxSize = \" << sgMaxSize\n",
    "              << \" sgSize = \" << sgSize << \" sId = \" << sId << \" j = \" << j\n",
    "              << \" k = \" << k << sycl::endl;\n",
    "        });\n",
    "  });\n",
    "  q.wait();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75001290-a11a-485f-89f9-57f48d4d687c",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8daf304-1c00-4a17-a7d9-cd6c4b556fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:02:41 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127233.v-qsvr-1           ..._data_type.sh u49991                 0 R batch          \n",
      "2127237.v-qsvr-1           ...g_max_size.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:19:27 AM PST\n",
      "#    Job ID:           2127237.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "globalId = 0 sgMaxSize = 16 sgSize = 15 sId = 0 j = 0 k = 15\n",
      "globalId = 1 sgMaxSize = 16 sgSize = 15 sId = 1 j = 1 k = 16\n",
      "globalId = 2 sgMaxSize = 16 sgSize = 15 sId = 2 j = 2 k = 17\n",
      "globalId = 3 sgMaxSize = 16 sgSize = 15 sId = 3 j = 3 k = 18\n",
      "globalId = 4 sgMaxSize = 16 sgSize = 15 sId = 4 j = 4 k = 19\n",
      "globalId = 5 sgMaxSize = 16 sgSize = 15 sId = 5 j = 5 k = 20\n",
      "globalId = 6 sgMaxSize = 16 sgSize = 15 sId = 6 j = 6 k = 21\n",
      "globalId = 7 sgMaxSize = 16 sgSize = 15 sId = 7 j = 7 k = 22\n",
      "globalId = 8 sgMaxSize = 16 sgSize = 15 sId = 8 j = 8 k = 23\n",
      "globalId = 9 sgMaxSize = 16 sgSize = 15 sId = 9 j = 9 k = 24\n",
      "globalId = 10 sgMaxSize = 16 sgSize = 15 sId = 10 j = 10 k = 25\n",
      "globalId = 11 sgMaxSize = 16 sgSize = 15 sId = 11 j = 11 k = 26\n",
      "globalId = 12 sgMaxSize = 16 sgSize = 15 sId = 12 j = 12 k = 27\n",
      "globalId = 13 sgMaxSize = 16 sgSize = 15 sId = 13 j = 13 k = 28\n",
      "globalId = 14 sgMaxSize = 16 sgSize = 15 sId = 14 j = 14 k = 29\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127237.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:19:40 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_max_size.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb0ec-488f-4c68-89ce-ab2c6b4bab5d",
   "metadata": {},
   "source": [
    "## Vectorization and Memory Access\n",
    "The Intel® graphics device has multiple EUs. Each EU is a multithreaded SIMD processor. The compiler generates SIMD instructions to pack multiple work-items in a sub-group to execute simultaneously in an EU thread. The SIMD width (thus the sub-group size), selected by the compiler is based on device characteristics and heuristics, or requested explicitly by the kernel, and can be 8, 16, or 32.\n",
    "\n",
    "Given a SIMD width, maximizing SIMD lane utilization gives optimal instruction performance. If one or more lanes (or kernel instances or work items) diverge, the thread executes both branch paths before the paths merge later, increasing the dynamic instruction count. SIMD divergence negatively impacts performance. The compiler works to minimize divergence, but it helps to avoid divergence in the source code, if possible.\n",
    "How memory is accessed in work-items affects how memory is accessed in the sub-group or how the SIMD lanes are utilized. Accessing contiguous memory in a work-item is often not optimal. \n",
    "\n",
    "For example: This kernel copies an array of 1024 x 1024 integers to another integer array of the same size. Each work-item copies 16 contiguous integers. However, the reads from data2 are gathered and stores to data are scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25109cba-fa2e-4f79-b189-29c36af19468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_mem_access_0.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_mem_access_0.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(sycl::nd_range(sycl::range{N / 16}, sycl::range{32}),\n",
    "                   [=](sycl::nd_item<1> it) {\n",
    "                     int i = it.get_global_linear_id();\n",
    "                     i = i * 16;\n",
    "                     for (int j = i; j < (i + 16); j++) {\n",
    "                       data[j] = data2[j];\n",
    "                     }\n",
    "                   });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info< sycl::info::event_profiling::command_end>() - e.template get_profiling_info< sycl::info::event_profiling::command_start>())<< \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed64100-dd31-455d-a273-bbf6d8743e5b",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bff6b0e-cccb-4a2f-b182-45e46c4c354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:02:45 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127233.v-qsvr-1           ..._data_type.sh u49991          00:00:31 R batch          \n",
      "2127240.v-qsvr-1           ...duction_sg.sh u49991                 0 R batch          \n",
      "2127241.v-qsvr-1           ...m_access_0.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:19:58 AM PST\n",
      "#    Job ID:           2127241.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "Kernel time = 1287164 ns\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127241.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:20:18 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_mem_access_0.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a3c97-4c63-421c-b8d7-d700bec9b399",
   "metadata": {},
   "source": [
    "It will be more efficient to change the code to read and store contiguous integers in each sub-group instead of each work-item. The example below does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302195bc-9731-4d6f-b8fd-c88fa39021da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_mem_access_1.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_mem_access_1.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(sycl::nd_range(sycl::range{N / 16}, sycl::range{32}),\n",
    "                   [=](sycl::nd_item<1> it) {\n",
    "                     int i = it.get_global_linear_id();\n",
    "                     sycl::ext::oneapi::sub_group sg = it.get_sub_group();\n",
    "                     int sgSize = sg.get_local_range()[0];\n",
    "                     i = (i / sgSize) * sgSize * 16 + (i % sgSize);\n",
    "                     for (int j = 0; j < sgSize * 16; j += sgSize) {\n",
    "                       data[i + j] = data2[i + j];\n",
    "                     }\n",
    "                   });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) << \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaacc4-da6f-4644-bcd5-377f9b63074e",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b62d79-c3c4-4efc-98a5-590676109b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:02:54 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127233.v-qsvr-1           ..._data_type.sh u49991          00:01:17 R batch          \n",
      "2127242.v-qsvr-1           ...ion_global.sh u49991                 0 R batch          \n",
      "2127243.v-qsvr-1           ...ction_sycl.sh u49991                 0 R batch          \n",
      "2127244.v-qsvr-1           ...m_access_1.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output █████████████████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:20:29 AM PST\n",
      "#    Job ID:           2127244.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "Kernel time = 357564 ns\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127244.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:20:44 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 41 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_mem_access_1.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3e8c2-c70e-4538-882d-5635b6c1d38b",
   "metadata": {},
   "source": [
    "Intel® graphics have instructions optimized for memory block loads/stores. So if work-items in a sub-group access a contiguous block of memory, you can use the sub-group block access functions to take advantage of these block load/store instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed16cb93-585f-41c7-9368-534c1af2822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_mem_access_2.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_mem_access_2.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range(sycl::range{N / 16}, sycl::range{32}), [=\n",
    "    ](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "          sycl::ext::oneapi::sub_group sg = it.get_sub_group();\n",
    "          sycl::vec<int, 8> x;\n",
    "\n",
    "          using global_ptr =\n",
    "              sycl::multi_ptr<int, sycl::access::address_space::global_space>;\n",
    "          int base = (it.get_group(0) * 32 +\n",
    "                      sg.get_group_id()[0] * sg.get_local_range()[0]) *\n",
    "                     16;\n",
    "          x = sg.load<8>(global_ptr(&(data2[base + 0])));\n",
    "          sg.store<8>(global_ptr(&(data[base + 0])), x);\n",
    "          x = sg.load<8>(global_ptr(&(data2[base + 128])));\n",
    "          sg.store<8>(global_ptr(&(data[base + 128])), x);\n",
    "        });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) << \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7ec1b-836a-4175-9c97-d2b464ed24bc",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b794b71-178f-40b3-8c65-918eb9b6bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:03:03 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127233.v-qsvr-1           ..._data_type.sh u49991          00:02:02 R batch          \n",
      "2127246.v-qsvr-1           ...ycl_blocks.sh u49991                 0 R batch          \n",
      "2127247.v-qsvr-1           ...lution_slm.sh u49991                 0 R batch          \n",
      "2127248.v-qsvr-1           ...m_access_2.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:21:11 AM PST\n",
      "#    Job ID:           2127248.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "Kernel time = 361050 ns\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127248.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:21:26 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_mem_access_2.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103f6e0-efb6-43ea-a7b6-fe1e823c6c52",
   "metadata": {},
   "source": [
    "## Data Sharing\n",
    "Because the work-items in a sub-group execute in the same thread, it is more efficient to share data between work-items, even if the data is private to each work-item. Sharing data in a sub-group is more efficient than sharing data in a work-group using shared local memory, or SLM. One way to share data among work-items in a sub-group is to use shuffle functions.\n",
    "\n",
    "This kernel transposes a 16 x 16 matrix. It looks more complicated than the previous examples, but the idea is simple: a sub-group loads a 16 x 16 sub-matrix, then the sub-matrix is transposed using the sub-group shuffle function `sycl::select_from_group(...)`. There is only one sub-matrix and the sub-matrix is the matrix so only one sub-group is needed. A bigger matrix, say 4096 x 4096, can be transposed using the same technique: each sub-group loads a sub-matrix, then the sub-matrices are transposed using the sub-group shuffle functions. This is left to the reader as an exercise.\n",
    "\n",
    "SYCL has multiple variants of sub-group shuffle functions available. Each variant is optimized for its specific purpose on specific devices. It is always a good idea to use these optimized functions (if they fit your needs) instead of creating your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeffe549-5be3-4535-b7ac-22faeecb1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sg_shuffle.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sg_shuffle.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iomanip>\n",
    "\n",
    "constexpr size_t N = 16;\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<unsigned int> matrix(N * N);\n",
    "  for (int i = 0; i < N * N; ++i) {\n",
    "    matrix[i] = i;\n",
    "  }\n",
    "\n",
    "  std::cout << \"Matrix: \" << std::endl;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    for (int j = 0; j < N; j++) {\n",
    "      std::cout << std::setw(3) << matrix[i * N + j] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "  }\n",
    "\n",
    "  {\n",
    "    constexpr size_t blockSize = 16;\n",
    "    sycl::buffer<unsigned int, 2> m(matrix.data(), sycl::range<2>(N, N));\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor marr(m, h);\n",
    "      sycl::local_accessor<unsigned int, 2> barr1(sycl::range<2>(blockSize, blockSize), h);\n",
    "      sycl::local_accessor<unsigned int, 2> barr2(sycl::range<2>(blockSize, blockSize), h);\n",
    "\n",
    "      h.parallel_for(\n",
    "          sycl::nd_range<2>(sycl::range<2>(N / blockSize, N),\n",
    "                            sycl::range<2>(1, blockSize)),\n",
    "          [=](sycl::nd_item<2> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "            int gi = it.get_group(0);\n",
    "            int gj = it.get_group(1);\n",
    "\n",
    "            sycl::sub_group sg = it.get_sub_group();\n",
    "            int sgId = sg.get_local_id()[0];\n",
    "\n",
    "            unsigned int bcol[blockSize];\n",
    "            int ai = blockSize * gi;\n",
    "            int aj = blockSize * gj;\n",
    "\n",
    "            for (int k = 0; k < blockSize; k++) {\n",
    "              bcol[k] = sg.load(marr.get_pointer() + (ai + k) * N + aj);\n",
    "            }\n",
    "\n",
    "            unsigned int tcol[blockSize];\n",
    "            for (int n = 0; n < blockSize; n++) {\n",
    "              if (sgId == n) {\n",
    "                for (int k = 0; k < blockSize; k++) {\n",
    "                  tcol[k] = sycl::select_from_group(sg, bcol[n], k);\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "\n",
    "            for (int k = 0; k < blockSize; k++) {\n",
    "              sg.store(marr.get_pointer() + (ai + k) * N + aj, tcol[k]);\n",
    "            }\n",
    "          });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_time = (e.template get_profiling_info< sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"\\nKernel Execution Time: \" << kernel_time * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  std::cout << std::endl << \"Transposed Matrix: \" << std::endl;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    for (int j = 0; j < N; j++) {\n",
    "      std::cout << std::setw(3) << matrix[i * N + j] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c5763-f7e9-4566-8e7d-2a7a395c6033",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f31097a-fadc-4a45-9e09-e6b5449ab2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2127059.v-qsvr-1           ...ub-singleuser u49991          00:03:03 R jupyterhub     \n",
      "2127071.v-qsvr-1           STDIN            u49991          00:02:23 R batch          \n",
      "2127233.v-qsvr-1           ..._data_type.sh u49991          00:02:02 R batch          \n",
      "2127250.v-qsvr-1           ...on_atomics.sh u49991                 0 R batch          \n",
      "2127251.v-qsvr-1           ..._data_type.sh u49991                 0 R batch          \n",
      "2127252.v-qsvr-1           ...am_256_int.sh u49991                 0 R batch          \n",
      "2127253.v-qsvr-1           ...sg_shuffle.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 12 Jan 2023 10:21:50 AM PST\n",
      "#    Job ID:           2127253.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "Matrix: \n",
      "  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 \n",
      " 16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31 \n",
      " 32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47 \n",
      " 48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63 \n",
      " 64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79 \n",
      " 80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95 \n",
      " 96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 \n",
      "112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 \n",
      "128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 \n",
      "144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 \n",
      "160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 \n",
      "176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 \n",
      "192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 \n",
      "208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 \n",
      "224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 \n",
      "240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 \n",
      "\n",
      "Kernel Execution Time: 0.014442 msec\n",
      "\n",
      "Transposed Matrix: \n",
      "  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 \n",
      "  1  17  33  49  65  81  97 113 129 145 161 177 193 209 225 241 \n",
      "  2  18  34  50  66  82  98 114 130 146 162 178 194 210 226 242 \n",
      "  3  19  35  51  67  83  99 115 131 147 163 179 195 211 227 243 \n",
      "  4  20  36  52  68  84 100 116 132 148 164 180 196 212 228 244 \n",
      "  5  21  37  53  69  85 101 117 133 149 165 181 197 213 229 245 \n",
      "  6  22  38  54  70  86 102 118 134 150 166 182 198 214 230 246 \n",
      "  7  23  39  55  71  87 103 119 135 151 167 183 199 215 231 247 \n",
      "  8  24  40  56  72  88 104 120 136 152 168 184 200 216 232 248 \n",
      "  9  25  41  57  73  89 105 121 137 153 169 185 201 217 233 249 \n",
      " 10  26  42  58  74  90 106 122 138 154 170 186 202 218 234 250 \n",
      " 11  27  43  59  75  91 107 123 139 155 171 187 203 219 235 251 \n",
      " 12  28  44  60  76  92 108 124 140 156 172 188 204 220 236 252 \n",
      " 13  29  45  61  77  93 109 125 141 157 173 189 205 221 237 253 \n",
      " 14  30  46  62  78  94 110 126 142 158 174 190 206 222 238 254 \n",
      " 15  31  47  63  79  95 111 127 143 159 175 191 207 223 239 255 \n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2127253.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 12 Jan 2023 10:22:04 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sg_shuffle.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40749ea0-81c4-447b-a6fc-5c1b8d1ac8d2",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
